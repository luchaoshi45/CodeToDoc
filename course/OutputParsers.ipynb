{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OutputParsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼•å…¥ä¾èµ–åŒ…ï¼Œè¿™é‡Œçš„ pydantic ç‰ˆæœ¬ä¸º v2\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain.tools import tool\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.output_parsers import RetryOutputParser, OutputFixingParser\n",
    "from typing import Dict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ä½¿ç”¨ deepseek\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0,\n",
    "    api_key=\"sk-6b2a3015b6094e68aa2956124ad1e870\",\n",
    "    api_base=\"https://api.deepseek.com\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ç”±äºæˆ‘æ— æ³•å®æ—¶è·å–ç½‘ç»œæ•°æ®ï¼Œæ— æ³•æä¾›åŒ—äº¬å½“å‰çš„å¤©æ°”æƒ…å†µã€‚ä¸è¿‡ï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è½»æ¾æŸ¥è¯¢ï¼š\\n\\n1. **å¤©æ°”åº”ç”¨**ï¼šæ‰“å¼€æ‰‹æœºä¸Šçš„å¤©æ°”åº”ç”¨ï¼ˆå¦‚è‹¹æœå¤©æ°”ã€å¢¨è¿¹å¤©æ°”ç­‰ï¼‰ï¼Œæœç´¢â€œåŒ—äº¬â€å³å¯æŸ¥çœ‹å®æ—¶å¤©æ°”ã€æ¸©åº¦ã€ç©ºæ°”è´¨é‡ç­‰è¯¦ç»†ä¿¡æ¯ã€‚\\n\\n2. **æœç´¢å¼•æ“**ï¼šåœ¨æµè§ˆå™¨ä¸­è¾“å…¥â€œåŒ—äº¬å®æ—¶å¤©æ°”â€ï¼Œæœç´¢ç»“æœä¼šç›´æ¥æ˜¾ç¤ºå½“å‰å¤©æ°”çŠ¶å†µï¼ˆå¦‚æ™´/é›¨ã€æ¸©åº¦ã€é£åŠ›ç­‰ï¼‰ã€‚\\n\\n3. **æ°”è±¡ç½‘ç«™**ï¼šè®¿é—®ä¸­å›½å¤©æ°”ç½‘ï¼ˆ[www.weather.com.cn](http://www.weather.com.cn)ï¼‰æˆ–ä¸­å¤®æ°”è±¡å°å®˜ç½‘è·å–æƒå¨é¢„æŠ¥ã€‚\\n\\n**æç¤º**ï¼šåŒ—äº¬è¿‘æœŸå¤„äºå¤å­£ï¼Œå¸¸è§é«˜æ¸©å¤šé›¨å¤©æ°”ï¼Œå»ºè®®å‡ºé—¨å‰å…³æ³¨ç´«å¤–çº¿æŒ‡æ•°å’Œé™æ°´æ¦‚ç‡ï¼Œåšå¥½é˜²æ™’æˆ–é˜²é›¨å‡†å¤‡ã€‚å¦‚éœ€æ›´å…·ä½“çš„ç©¿è¡£æˆ–å‡ºè¡Œå»ºè®®ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘ä½ çš„éœ€æ±‚å“¦ï¼ â˜€ï¸ğŸŒ§ï¸', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 182, 'prompt_tokens': 8, 'total_tokens': 190, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 8}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0623_fp8_kvcache', 'id': '1b1b1507-7176-4365-a0c2-76adc7e78b41', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--61b68fb4-9676-430c-a5d5-144362451ff6-0', usage_metadata={'input_tokens': 8, 'output_tokens': 182, 'total_tokens': 190, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"åŒ—äº¬ç°åœ¨çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 æµ‹è¯• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_weather` with `{'location': 'åŒ—äº¬'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3må¤©æ°”æ™´æœ—ï¼ŒåŒ—äº¬å½“å‰æ¸©åº¦ä¸º22æ‘„æ°åº¦\u001b[0m\u001b[32;1m\u001b[1;3måŒ—äº¬ç°åœ¨çš„å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦ä¸º22æ‘„æ°åº¦ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "æœ€ç»ˆå›ç­”ï¼š åŒ—äº¬ç°åœ¨çš„å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦ä¸º22æ‘„æ°åº¦ã€‚\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# 1. å®šä¹‰å·¥å…·\n",
    "# =======================\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    æ ¹æ®åœ°åè¿”å›æ¨¡æ‹Ÿå¤©æ°”ä¿¡æ¯ã€‚\n",
    "    \"\"\"\n",
    "    return f\"å¤©æ°”æ™´æœ—ï¼Œ{location}å½“å‰æ¸©åº¦ä¸º22æ‘„æ°åº¦\"\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 3. åˆ›å»º promptï¼ˆå¿…é¡»åŒ…å«å·¥å…·å ä½ç¬¦ï¼‰\n",
    "# =======================\n",
    "# æ¨èä½¿ç”¨å®˜æ–¹ agent prompt æˆ–è‡ªå®šä¹‰æ”¯æŒå·¥å…·è°ƒç”¨çš„ prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚å¦‚æœé—®é¢˜æ¶‰åŠå¤©æ°”ï¼Œè¯·ä½¿ç”¨å·¥å…·æŸ¥è¯¢åå†å›ç­”ã€‚\"),\n",
    "    # ç§»é™¤äº† chat_history\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),  # å¿…é¡»ä¿ç•™\n",
    "])\n",
    "\n",
    "# =======================\n",
    "# 4. åˆ›å»º Agent\n",
    "# =======================\n",
    "# ç»‘å®šå·¥å…·å¹¶åˆ›å»º agent\n",
    "agent = create_tool_calling_agent(\n",
    "    llm=llm,\n",
    "    tools=[get_weather],\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# 5. åˆ›å»ºæ‰§è¡Œå™¨ï¼ˆAgentExecutorï¼‰\n",
    "# =======================\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=[get_weather],\n",
    "    verbose=True,  # æ˜¾ç¤ºä¸­é—´æ­¥éª¤\n",
    "    handle_parsing_errors=True,  # è‡ªåŠ¨å¤„ç†è§£æé”™è¯¯\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# 6. è°ƒç”¨æµ‹è¯•\n",
    "# =======================\n",
    "response = agent_executor.invoke({\n",
    "    \"input\": \"åŒ—äº¬ç°åœ¨çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\"\n",
    "})\n",
    "\n",
    "print(\"\\næœ€ç»ˆå›ç­”ï¼š\", response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 è§£æå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ä¸€ä¸ªåä¸ºJokeçš„æ•°æ®æ¨¡å‹\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"ç¬‘è¯ä¸­çš„é“ºå«é—®é¢˜ï¼Œå¿…é¡»ä»¥ï¼Ÿç»“å°¾\")\n",
    "    punchline: str = Field(description=\"ç¬‘è¯ä¸­å›ç­”é“ºå«é—®é¢˜çš„éƒ¨åˆ†ï¼Œé€šå¸¸æ˜¯ä¸€ç§æŠ–åŒ…è¢±æ–¹å¼å›ç­”é“ºå«é—®é¢˜ï¼Œä¾‹å¦‚è°éŸ³ã€ä¼šé”™æ„ç­‰\")\n",
    "\n",
    "    # éªŒè¯å™¨ï¼Œç¡®ä¿setupå­—æ®µä»¥è‹±æ–‡æˆ–ä¸­æ–‡é—®å·ç»“å°¾\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def question_ends_with_question_mark(cls, values: Dict) -> Dict:\n",
    "        setup = values.get(\"setup\")\n",
    "        if setup:\n",
    "            # æ£€æŸ¥æ˜¯å¦ä»¥è‹±æ–‡é—®å·(?)æˆ–ä¸­æ–‡é—®å·(ï¼Ÿ)ç»“å°¾\n",
    "            if not (setup.endswith(\"?\") or setup.endswith(\"ï¼Ÿ\")):\n",
    "                raise ValueError(\"Badly formed question! The setup must end with a question mark (? or ï¼Ÿ).\")\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: content='```json\\n{\\n  \"setup\": \"ä¸ºä»€ä¹ˆæ•°å­¦ä¹¦æ€»æ˜¯å¾ˆå¿§éƒï¼Ÿ\",\\n  \"punchline\": \"å› ä¸ºå®ƒæœ‰å¤ªå¤šçš„é—®é¢˜ï¼\"\\n}\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 227, 'total_tokens': 259, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 192}, 'prompt_cache_hit_tokens': 192, 'prompt_cache_miss_tokens': 35}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0623_fp8_kvcache', 'id': '8dd76625-b57f-4d99-ba1d-b777781c655c', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--91f02c02-5181-49a1-aa0b-9c43e9d00701-0' usage_metadata={'input_tokens': 227, 'output_tokens': 32, 'total_tokens': 259, 'input_token_details': {'cache_read': 192}, 'output_token_details': {}}\n",
      "Parsed output: setup='ä¸ºä»€ä¹ˆæ•°å­¦ä¹¦æ€»æ˜¯å¾ˆå¿§éƒï¼Ÿ' punchline='å› ä¸ºå®ƒæœ‰å¤ªå¤šçš„é—®é¢˜ï¼'\n"
     ]
    }
   ],
   "source": [
    "# å®ä¾‹åŒ–è§£æå™¨ã€æç¤ºè¯æ¨¡æ¿\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"å›ç­”ç”¨æˆ·çš„æŸ¥è¯¢.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨LCELè¯­æ³•ç»„åˆä¸€ä¸ªç®€å•çš„é“¾\n",
    "prompt_and_model = prompt | llm\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹å¹¶è§£æè¾“å‡º\n",
    "output = prompt_and_model.invoke({\"query\": \"ç»™æˆ‘è®²ä¸€ä¸ªç¬‘è¯\"})\n",
    "print(\"Model output:\", output)\n",
    "\n",
    "try:\n",
    "    parsed_output = parser.invoke(output)\n",
    "    print(\"Parsed output:\", parsed_output)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to parse output: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: content='```json\\n{\\n  \"joke\": \"ä¸ºä»€ä¹ˆç”µè„‘ç»å¸¸æ„Ÿå†’ï¼Ÿå› ä¸ºå®ƒæ€»æ˜¯å¼€ç€çª—æˆ·ï¼ˆWindowsï¼‰ï¼\"\\n}\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 17, 'total_tokens': 43, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 17}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0623_fp8_kvcache', 'id': '7b1b74ce-0114-4cd3-8d20-2eec51ef3d51', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--84b0e994-0c80-464e-a3ac-038846b7c7ae-0' usage_metadata={'input_tokens': 17, 'output_tokens': 26, 'total_tokens': 43, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n",
      "Parsed output (JSON format): {'joke': 'ä¸ºä»€ä¹ˆç”µè„‘ç»å¸¸æ„Ÿå†’ï¼Ÿå› ä¸ºå®ƒæ€»æ˜¯å¼€ç€çª—æˆ·ï¼ˆWindowsï¼‰ï¼'}\n"
     ]
    }
   ],
   "source": [
    "# å®ä¾‹åŒ–è§£æå™¨ã€æç¤ºè¯æ¨¡æ¿\n",
    "parser = JsonOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"å›ç­”ç”¨æˆ·çš„æŸ¥è¯¢.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨LCELè¯­æ³•ç»„åˆä¸€ä¸ªç®€å•çš„é“¾\n",
    "prompt_and_model = prompt | llm\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹å¹¶è§£æè¾“å‡º\n",
    "output = prompt_and_model.invoke({\"query\": \"ç»™æˆ‘è®²ä¸€ä¸ªç¬‘è¯\"})\n",
    "print(\"Model output:\", output)\n",
    "\n",
    "try:\n",
    "    parsed_output = parser.parse(output.content)\n",
    "    print(\"Parsed output (JSON format):\", parsed_output)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to parse output: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action='tell_joke' action_input=''action='tell_joke' action_input='Why'action='tell_joke' action_input='Why do'action='tell_joke' action_input='Why do programmers'action='tell_joke' action_input='Why do programmers prefer'action='tell_joke' action_input='Why do programmers prefer dark'action='tell_joke' action_input='Why do programmers prefer dark mode'action='tell_joke' action_input='Why do programmers prefer dark mode?'action='tell_joke' action_input='Why do programmers prefer dark mode? Because'action='tell_joke' action_input='Why do programmers prefer dark mode? Because light'action='tell_joke' action_input='Why do programmers prefer dark mode? Because light attracts'action='tell_joke' action_input='Why do programmers prefer dark mode? Because light attracts bugs'action='tell_joke' action_input='Why do programmers prefer dark mode? Because light attracts bugs!'"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨LCELè¯­æ³•ç»„åˆä¸€ä¸ªç®€å•çš„é“¾\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# æµå¼è°ƒç”¨å¹¶é€ä¸ªæ‰“å°è¾“å‡ºç‰‡æ®µ\n",
    "for s in chain.stream({\"query\": \"ç»™æˆ‘è®²ä¸€ä¸ªå…³äºç¨‹åºå‘˜ç¼–ç¨‹çš„ç¬‘è¯\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 LLMåº”ç”¨å®¹é”™æœºåˆ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ­£å¸¸æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Answer the user query.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"action\": {\"description\": \"action to take\", \"title\": \"Action\", \"type\": \"string\"}, \"action_input\": {\"description\": \"input to the action\", \"title\": \"Action Input\", \"type\": \"string\"}}, \"required\": [\"action\", \"action_input\"]}\\n```\\nåŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿ\\n'\n",
      "Model output: content='```json\\n{\\n  \"action\": \"tell_joke\",\\n  \"action_input\": \"ä¸ºä»€ä¹ˆç”µè„‘å¾ˆç¬¨ï¼Ÿå› ä¸ºå®ƒæ€»æ˜¯è¢«äººâ€˜ç‚¹â€™æ¥â€˜ç‚¹â€™å»ï¼\"\\n}\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 200, 'total_tokens': 239, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 192}, 'prompt_cache_hit_tokens': 192, 'prompt_cache_miss_tokens': 8}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0623_fp8_kvcache', 'id': 'aac6431f-afba-4ca8-a334-5cf78d94df9a', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--fd369b63-5c0a-4523-982f-0598e3e7b0a6-0' usage_metadata={'input_tokens': 200, 'output_tokens': 39, 'total_tokens': 239, 'input_token_details': {'cache_read': 192}, 'output_token_details': {}}\n",
      "Parsed output (JSON format): action='tell_joke' action_input='ä¸ºä»€ä¹ˆç”µè„‘å¾ˆç¬¨ï¼Ÿå› ä¸ºå®ƒæ€»æ˜¯è¢«äººâ€˜ç‚¹â€™æ¥â€˜ç‚¹â€™å»ï¼'\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Based on the user question, provide an Action and Action Input for what step should be taken.\n",
    "{format_instructions}\n",
    "Question: {query}\n",
    "Response:\"\"\"\n",
    "\n",
    "class Action(BaseModel):\n",
    "    action: str = Field(description=\"action to take\")\n",
    "    action_input: str = Field(description=\"input to the action\")\n",
    "parser = PydanticOutputParser(pydantic_object=Action)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = \"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "prompt_values = prompt.format_prompt(query=\"åŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿ\")\n",
    "print(prompt_values)\n",
    "\n",
    "# ä½¿ç”¨LCELè¯­æ³•ç»„åˆä¸€ä¸ªç®€å•çš„é“¾\n",
    "prompt_and_model = prompt | llm\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹å¹¶è§£æè¾“å‡º\n",
    "output = prompt_and_model.invoke({\"query\": \"ç»™æˆ‘è®²ä¸€ä¸ªç¬‘è¯\"})\n",
    "print(\"Model output:\", output)\n",
    "\n",
    "try:\n",
    "    parsed_output = parser.parse(output.content)\n",
    "    print(\"Parsed output (JSON format):\", parsed_output)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to parse output: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RetryOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse output: Failed to parse Action from completion {\"action\": \"search\"}. Got: 1 validation error for Action\n",
      "action_input\n",
      "  Field required [type=missing, input_value={'action': 'search'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "\n",
      "parser=PydanticOutputParser(pydantic_object=<class '__main__.Action'>) retry_chain=PromptTemplate(input_variables=['completion', 'prompt'], input_types={}, partial_variables={}, template='Prompt:\\n{prompt}\\nCompletion:\\n{completion}\\n\\nAbove, the Completion did not satisfy the constraints given in the Prompt.\\nPlease try again:')\n",
      "| ChatDeepSeek(client=<openai.resources.chat.completions.completions.Completions object at 0x000001F4D413D280>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F4D413E660>, root_client=<openai.OpenAI object at 0x000001F4D5134E90>, root_async_client=<openai.AsyncOpenAI object at 0x000001F4D550AF30>, model_name='deepseek-chat', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), api_key=SecretStr('**********'), api_base='https://api.deepseek.com')\n",
      "| StrOutputParser()\n",
      "\n",
      "\n",
      "action='search' action_input='åŒ—äº¬å¤©æ°”'\n"
     ]
    }
   ],
   "source": [
    "bad_response = '{\"action\": \"search\"}'\n",
    "try:\n",
    "    parsed_output = parser.parse(bad_response)\n",
    "    print(\"Parsed output (JSON format):\", parsed_output)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to parse output: {e}\\n\")\n",
    "    # å¤„ç†\n",
    "    retry_parser = RetryOutputParser.from_llm(\n",
    "        parser = parser,\n",
    "        llm=llm, # å¯ä»¥æ›´æ”¹æ–°çš„llm\n",
    "    )\n",
    "    print(retry_parser)\n",
    "    print(\"\\n\")\n",
    "    print(retry_parser.parse_with_prompt(bad_response, prompt_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OutputFixingParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse output: Failed to parse Action from completion {\"action\": \"search\"}. Got: 1 validation error for Action\n",
      "action_input\n",
      "  Field required [type=missing, input_value={'action': 'search'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "\n",
      "parser=PydanticOutputParser(pydantic_object=<class '__main__.Action'>) retry_chain=PromptTemplate(input_variables=['completion', 'error', 'instructions'], input_types={}, partial_variables={}, template='Instructions:\\n--------------\\n{instructions}\\n--------------\\nCompletion:\\n--------------\\n{completion}\\n--------------\\n\\nAbove, the Completion did not satisfy the constraints given in the Instructions.\\nError:\\n--------------\\n{error}\\n--------------\\n\\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions:')\n",
      "| ChatDeepSeek(client=<openai.resources.chat.completions.completions.Completions object at 0x000001F4D2CE6570>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F4D3EF2180>, root_client=<openai.OpenAI object at 0x000001F4D2CE7AA0>, root_async_client=<openai.AsyncOpenAI object at 0x000001F4D3EF0890>, model_name='deepseek-chat', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), api_key=SecretStr('**********'), api_base='https://api.deepseek.com')\n",
      "| StrOutputParser()\n",
      "\n",
      "\n",
      "action='search' action_input='query'\n",
      "\n",
      "\n",
      "action='search' action_input='query'\n"
     ]
    }
   ],
   "source": [
    "bad_response = '{\"action\": \"search\"}'\n",
    "try:\n",
    "    parsed_output = parser.parse(bad_response)\n",
    "    print(\"Parsed output (JSON format):\", parsed_output)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to parse output: {e}\\n\")\n",
    "    # å¤„ç†\n",
    "    fix_parser = OutputFixingParser.from_llm(\n",
    "        parser = parser,\n",
    "        llm=llm, # å¯ä»¥æ›´æ”¹æ–°çš„llm\n",
    "    )\n",
    "    print(fix_parser)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(fix_parser.parse(bad_response))\n",
    "    print(\"\\n\")\n",
    "    print(fix_parser.parse_with_prompt(bad_response, prompt_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è‡ªå®šä¹‰è§£æå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 'YES' to: True\n",
      "Parsed 'NO' to: False\n",
      "Error parsing 'MEOW': BooleanOutputParser expected output value to either be YES or NO (case-insensitive). Received MEOW.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from langchain_core.output_parsers import BaseOutputParser  # ä» langchain_core å¯¼å…¥\n",
    "from langchain_core.exceptions import OutputParserException  # ä» langchain_core.exceptions å¯¼å…¥\n",
    "from pydantic import BaseModel, Field  # å¯¼å…¥ Pydantic çš„ BaseModel å’Œ Field\n",
    "\n",
    "# è‡ªå®šä¹‰å¸ƒå°”å€¼è§£æå™¨\n",
    "class BooleanOutputParser(BaseOutputParser[bool]):\n",
    "    \"\"\"Custom boolean parser.\"\"\"\n",
    "\n",
    "    # å®šä¹‰ Pydantic å­—æ®µ\n",
    "    true_val: str = Field(default=\"YES\", description=\"è¡¨ç¤º True çš„å­—ç¬¦ä¸²\")\n",
    "    false_val: str = Field(default=\"NO\", description=\"è¡¨ç¤º False çš„å­—ç¬¦ä¸²\")\n",
    "\n",
    "    def parse(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        å°†è¾“å…¥çš„æ–‡æœ¬è§£æä¸ºå¸ƒå°”å€¼ã€‚\n",
    "        \n",
    "        :param text: è¾“å…¥çš„å­—ç¬¦ä¸²ã€‚\n",
    "        :return: è§£æåçš„å¸ƒå°”å€¼ã€‚\n",
    "        :raises OutputParserException: å¦‚æœè¾“å…¥çš„å­—ç¬¦ä¸²æ—¢ä¸æ˜¯ true_val ä¹Ÿä¸æ˜¯ false_valã€‚\n",
    "        \"\"\"\n",
    "        cleaned_text = text.strip().upper()  # å»é™¤ç©ºæ ¼å¹¶è½¬æ¢ä¸ºå¤§å†™\n",
    "        if cleaned_text not in (self.true_val.upper(), self.false_val.upper()):\n",
    "            raise OutputParserException(\n",
    "                f\"BooleanOutputParser expected output value to either be \"\n",
    "                f\"{self.true_val} or {self.false_val} (case-insensitive). \"\n",
    "                f\"Received {cleaned_text}.\"\n",
    "            )\n",
    "        return cleaned_text == self.true_val.upper()\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        \"\"\"\n",
    "        è¿”å›è§£æå™¨çš„ç±»å‹åç§°ã€‚\n",
    "        \n",
    "        :return: è§£æå™¨çš„ç±»å‹åç§°ã€‚\n",
    "        \"\"\"\n",
    "        return \"boolean_output_parser\"\n",
    "\n",
    "# æµ‹è¯•è§£æå™¨\n",
    "if __name__ == \"__main__\":\n",
    "    # åˆ›å»ºè§£æå™¨å®ä¾‹\n",
    "    parser = BooleanOutputParser(true_val=\"YES\", false_val=\"NO\")\n",
    "\n",
    "    # æ­£å¸¸è§£æ\n",
    "    try:\n",
    "        result = parser.parse(\"YES\")\n",
    "        print(f\"Parsed 'YES' to: {result}\")  # è¾“å‡º: Parsed 'YES' to: True\n",
    "    except OutputParserException as e:\n",
    "        print(f\"Error parsing 'YES': {e}\")\n",
    "\n",
    "    try:\n",
    "        result = parser.parse(\"NO\")\n",
    "        print(f\"Parsed 'NO' to: {result}\")  # è¾“å‡º: Parsed 'NO' to: False\n",
    "    except OutputParserException as e:\n",
    "        print(f\"Error parsing 'NO': {e}\")\n",
    "\n",
    "    # é”™è¯¯è§£æ\n",
    "    try:\n",
    "        result = parser.parse(\"MEOW\")\n",
    "        print(f\"Parsed 'MEOW' to: {result}\")\n",
    "    except OutputParserException as e:\n",
    "        print(f\"Error parsing 'MEOW': {e}\")  # è¾“å‡º: Error parsing 'MEOW': ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
