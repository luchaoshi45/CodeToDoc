{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PromptTemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import os\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, ChatMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# DashScope API Key\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-0738f7176f0a44e8ae8bc1569c2b6032\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–æ¨¡åž‹\n",
    "llm = Tongyi(\n",
    "    model_name=\"qwen-coder-plus\",\n",
    "    dashscope_api_key=os.environ[\"DASHSCOPE_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªèµ·åå¤§å¸ˆ', additional_kwargs={'name': 'é™ˆå¤§å¸ˆ'}, response_metadata={}),\n",
       " HumanMessage(content='è¯·é—®ä½ å«ä»€ä¹ˆ', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ä½ å¥½ï¼Œæˆ‘æ˜¯é™ˆå¤§å¸ˆ', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = SystemMessage(\n",
    "    content=\"ä½ æ˜¯ä¸€ä¸ªèµ·åå¤§å¸ˆ\",\n",
    "    additional_kwargs={\"name\": \"é™ˆå¤§å¸ˆ\"}\n",
    ")\n",
    "human_message = HumanMessage(content=\"è¯·é—®ä½ å«ä»€ä¹ˆ\")\n",
    "ai_message = AIMessage(content=\"ä½ å¥½ï¼Œæˆ‘æ˜¯é™ˆå¤§å¸ˆ\")\n",
    "[system_message, human_message, ai_message]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ æ˜¯ä¸€ä¸ªç®—å‘½å¤§å¸ˆ,å¸®æˆ‘èµ·1ä¸ªå…·æœ‰æ³•å›½ç‰¹è‰²çš„å¥³å­©åå­—'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"ä½ æ˜¯ä¸€ä¸ª{name},å¸®æˆ‘èµ·1ä¸ªå…·æœ‰{county}ç‰¹è‰²çš„{sex}åå­—\")\n",
    "prompt.format(name=\"ç®—å‘½å¤§å¸ˆ\",county=\"æ³•å›½\",sex=\"å¥³å­©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['name', 'user_input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='ä½ æ˜¯ä¸€ä¸ªèµ·åå¤§å¸ˆ. ä½ çš„åå­—å«{name}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='ä½ å¥½{name},ä½ æ„Ÿè§‰å¦‚ä½•ï¼Ÿ'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ä½ å¥½ï¼æˆ‘çŠ¶æ€éžå¸¸å¥½!'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ä½ å«ä»€ä¹ˆåå­—å‘¢?'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='ä½ å¥½ï¼æˆ‘å«{name}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ä½ æ˜¯ä¸€ä¸ªèµ·åå¤§å¸ˆ. ä½ çš„åå­—å«{name}.\"),\n",
    "        (\"human\", \"ä½ å¥½{name},ä½ æ„Ÿè§‰å¦‚ä½•ï¼Ÿ\"),\n",
    "        (\"ai\", \"ä½ å¥½ï¼æˆ‘çŠ¶æ€éžå¸¸å¥½!\"),\n",
    "        (\"human\", \"ä½ å«ä»€ä¹ˆåå­—å‘¢?\"),\n",
    "        (\"ai\", \"ä½ å¥½ï¼æˆ‘å«{name}\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_template.format_messages(name=\"é™ˆå¤§å¸ˆ\", user_input=\"ä½ çš„çˆ¸çˆ¸æ˜¯è°å‘¢?\")\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='æ„¿åŽŸåŠ›ä¸Žä½ åŒåœ¨ï¼', additional_kwargs={}, response_metadata={}, role='å¤©è¡Œè€…')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"æ„¿{subject}ä¸Žä½ åŒåœ¨ï¼\"\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(role=\"å¤©è¡Œè€…\",template=prompt)\n",
    "chat_message_prompt.format(subject=\"åŽŸåŠ›\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Customized templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['function_name'] input_types={} partial_variables={}\n",
      "\n",
      "            ä½ æ˜¯ä¸€ä¸ªéžå¸¸æœ‰ç»éªŒå’Œå¤©èµ‹çš„ç¨‹åºå‘˜ï¼ŒçŽ°åœ¨ç»™ä½ å¦‚ä¸‹å‡½æ•°åç§°ï¼Œä½ ä¼šæŒ‰ç…§å¦‚ä¸‹æ ¼å¼ï¼Œè¾“å‡ºè¿™æ®µä»£ç çš„åç§°ã€æºä»£ç ã€ä¸­æ–‡è§£é‡Šã€‚\n",
      "            å‡½æ•°åç§°: hello_world\n",
      "            æºä»£ç :\n",
      "            def hello_world(abc):\n",
      "    print(\"Hello, world!\")\n",
      "    return abc\n",
      "\n",
      "            ä»£ç è§£é‡Š:\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'å½“ç„¶å¯ä»¥ï¼ä»¥ä¸‹æ˜¯æŒ‰ç…§ä½ æä¾›çš„æ ¼å¼è¾“å‡ºçš„å†…å®¹ï¼š\\n\\n---\\n\\n**ä»£ç åç§°**: `hello_world`\\n\\n**æºä»£ç **:\\n```python\\ndef hello_world(abc):\\n    print(\"Hello, world!\")\\n    return abc\\n```\\n\\n**ä¸­æ–‡è§£é‡Š**:  \\nè¿™æ˜¯ä¸€ä¸ªåä¸º `hello_world` çš„å‡½æ•°ï¼Œå®ƒæŽ¥æ”¶ä¸€ä¸ªå‚æ•° `abc`ã€‚å‡½æ•°çš„åŠŸèƒ½æ˜¯æ‰“å°å­—ç¬¦ä¸² `\"Hello, world!\"`ï¼Œç„¶åŽå°†ä¼ å…¥çš„å‚æ•° `abc` åŽŸæ ·è¿”å›žã€‚è¿™ä¸ªå‡½æ•°ä¸»è¦ç”¨äºŽæ¼”ç¤ºåŸºæœ¬çš„å‡½æ•°ç»“æž„å’Œè¾“å‡ºåŠŸèƒ½ã€‚è™½ç„¶å‚æ•° `abc` åœ¨å‡½æ•°ä½“ä¸­æ²¡æœ‰è¢«ä½¿ç”¨ï¼Œä½†å®ƒä¼šè¢«ç›´æŽ¥è¿”å›žï¼Œå› æ­¤è°ƒç”¨è€…å¯ä»¥ä¼ å…¥ä»»ä½•å€¼ä½œä¸ºæµ‹è¯•ã€‚'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import StringPromptTemplate\n",
    "import inspect\n",
    "\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    def format(self, **kwargs) -> str:\n",
    "        prompt_const = \"\"\"\n",
    "            ä½ æ˜¯ä¸€ä¸ªéžå¸¸æœ‰ç»éªŒå’Œå¤©èµ‹çš„ç¨‹åºå‘˜ï¼ŒçŽ°åœ¨ç»™ä½ å¦‚ä¸‹å‡½æ•°åç§°ï¼Œä½ ä¼šæŒ‰ç…§å¦‚ä¸‹æ ¼å¼ï¼Œè¾“å‡ºè¿™æ®µä»£ç çš„åç§°ã€æºä»£ç ã€ä¸­æ–‡è§£é‡Šã€‚\n",
    "            å‡½æ•°åç§°: {function_name}\n",
    "            æºä»£ç :\n",
    "            {source_code}\n",
    "            ä»£ç è§£é‡Š:\n",
    "        \"\"\"\n",
    "        source_code = inspect.getsource(kwargs[\"function_name\"])\n",
    "        prompt = prompt_const.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__, \n",
    "            source_code=source_code\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "def hello_world(abc):\n",
    "    print(\"Hello, world!\")\n",
    "    return abc\n",
    "    \n",
    "cpt = CustomPromptTemplate(input_variables=[\"function_name\"])\n",
    "print(cpt)\n",
    "print(cpt.format(function_name=hello_world))\n",
    "\n",
    "chain = cpt | llm\n",
    "chain.invoke({\n",
    "    \"function_name\": hello_world\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. ä¸‰å±‚æç¤ºè¯è®¾è®¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['behavior', 'character', 'prohibit'] input_types={} partial_variables={} template='\\n{character}\\n{behavior}\\n{prohibit}\\n'\n",
      "input_variables=['person', 'xingge', 'prohibit_list', 'behavior_list'] input_types={} partial_variables={} final_prompt=PromptTemplate(input_variables=['behavior', 'character', 'prohibit'], input_types={}, partial_variables={}, template='\\n{character}\\n{behavior}\\n{prohibit}\\n') pipeline_prompts=[('character', PromptTemplate(input_variables=['person', 'xingge'], input_types={}, partial_variables={}, template='ä½ æ˜¯{person}ï¼Œä½ æœ‰ç€{xingge}.')), ('behavior', PromptTemplate(input_variables=['behavior_list'], input_types={}, partial_variables={}, template='ä½ éµä»Žä»¥ä¸‹çš„è¡Œä¸º:\\n{behavior_list}\\n')), ('prohibit', PromptTemplate(input_variables=['prohibit_list'], input_types={}, partial_variables={}, template='ä½ ä¸å…è®¸æœ‰ä»¥ä¸‹è¡Œä¸º:\\n{prohibit_list}\\n'))]\n",
      "\n",
      "ä½ æ˜¯åŸƒéš†é©¬æ–¯å…‹ï¼Œä½ æœ‰ç€é’¢é“èˆ¬çš„æ„å¿—ï¼Œä½ æ˜¯ä¸–ç•Œé¦–å¯Œï¼Œç²¾é€šç‰©ç†ã€å¤©æ–‡ã€å•†ä¸šï¼Œä½ æ˜¯å•†ä¸šå¥‡æ‰ï¼Œä½ çš„ç»ˆæžæ¢¦æƒ³æ˜¯æ®–æ°‘ç«æ˜Ÿ..\n",
      "ä½ éµä»Žä»¥ä¸‹çš„è¡Œä¸º:\n",
      "1.ä½ å–œæ¬¢å†’é™© \n",
      "2.ä½ éžå¸¸å´‡æ‹œçˆ±å› æ–¯å¦ \n",
      "3.ä½ æ€»æ˜¯ä¹äºŽåŠ©äººï¼Œå›žç­”ç”¨æˆ·å„ç§é—®é¢˜.\n",
      "\n",
      "ä½ ä¸å…è®¸æœ‰ä»¥ä¸‹è¡Œä¸º:\n",
      "1.ä½ ä¸å¯ä»¥è¯´è‡ªå·±æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹æˆ–è€…æœºå™¨äºº \n",
      "2.å½“ä½ ä¸çŸ¥é“ç­”æ¡ˆçš„æ—¶å€™ï¼Œå¯ä»¥è¯´è®©æˆ‘å†æƒ³æƒ³. \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_8852\\2121411173.py:27: LangChainDeprecationWarning: This class is deprecated in favor of chaining individual prompts together.\n",
      "  pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)\n"
     ]
    }
   ],
   "source": [
    "# ä¸‰å¼•å·å­—ç¬¦ä¿ç•™ç©ºæ ¼å’Œæ¢è¡Œ\n",
    "full_template = \"\"\"\n",
    "{character}\n",
    "{behavior}\n",
    "{prohibit}\n",
    "\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "print(full_prompt)\n",
    "\n",
    "from langchain_core.prompts import PipelinePromptTemplate\n",
    "character_template = \"\"\"ä½ æ˜¯{person}ï¼Œä½ æœ‰ç€{xingge}.\"\"\"\n",
    "character_prompt = PromptTemplate.from_template(character_template)\n",
    "behavior_template = \"\"\"ä½ éµä»Žä»¥ä¸‹çš„è¡Œä¸º:\n",
    "{behavior_list}\n",
    "\"\"\"\n",
    "behavior_prompt = PromptTemplate.from_template(behavior_template)\n",
    "prohibit_template = \"\"\"ä½ ä¸å…è®¸æœ‰ä»¥ä¸‹è¡Œä¸º:\n",
    "{prohibit_list}\n",
    "\"\"\"\n",
    "prohibit_prompt = PromptTemplate.from_template(prohibit_template)\n",
    "\n",
    "input_prompts = [\n",
    "    (\"character\", character_prompt),\n",
    "    (\"behavior\", behavior_prompt),\n",
    "    (\"prohibit\", prohibit_prompt)\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)\n",
    "print(pipeline_prompt)\n",
    "\n",
    "\n",
    "pm = pipeline_prompt.format(\n",
    "    person=\"åŸƒéš†é©¬æ–¯å…‹\",\n",
    "    xingge=\"é’¢é“èˆ¬çš„æ„å¿—ï¼Œä½ æ˜¯ä¸–ç•Œé¦–å¯Œï¼Œç²¾é€šç‰©ç†ã€å¤©æ–‡ã€å•†ä¸šï¼Œä½ æ˜¯å•†ä¸šå¥‡æ‰ï¼Œä½ çš„ç»ˆæžæ¢¦æƒ³æ˜¯æ®–æ°‘ç«æ˜Ÿ.\",\n",
    "    behavior_list=\"1.ä½ å–œæ¬¢å†’é™© \\n2.ä½ éžå¸¸å´‡æ‹œçˆ±å› æ–¯å¦ \\n3.ä½ æ€»æ˜¯ä¹äºŽåŠ©äººï¼Œå›žç­”ç”¨æˆ·å„ç§é—®é¢˜.\",\n",
    "    prohibit_list=\"1.ä½ ä¸å¯ä»¥è¯´è‡ªå·±æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹æˆ–è€…æœºå™¨äºº \\n2.å½“ä½ ä¸çŸ¥é“ç­”æ¡ˆçš„æ—¶å€™ï¼Œå¯ä»¥è¯´è®©æˆ‘å†æƒ³æƒ³. \"\n",
    ")\n",
    "print(pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. åºåˆ—åŒ–ï¼šä½¿ç”¨æ–‡ä»¶æ¥ç®¡ç†æç¤ºè¯æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['name', 'what'] input_types={} partial_variables={} template='test{name}test{what}test'\n",
      "testå°çº¢testæžç¬‘çš„test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'å“ˆå“ˆå“ˆï¼Œä½ è¿™æ˜¯åœ¨æµ‹è¯•â€œtestâ€çš„æžç¬‘æžé™å—ï¼Ÿ  \\ntestå°çº¢testæžç¬‘çš„testï¼Œç»“æžœtestè¢«ä½ çŽ©æˆä¸‰æ–‡é±¼äº†ðŸŸ  \\nï¼ˆæˆ˜æœ¯åŽä»°ï¼‰å†testä¸‹åŽ»ï¼Œç³»ç»Ÿå°±è¦æŠ¥è­¦å•¦ï½ž  \\n\\nè¦ä¸å’±æ­£ç»ç‚¹ï¼Ÿæ¯”å¦‚â€”â€”  \\nâ€œtestå°çº¢testæžç¬‘çš„testâ€  \\nâ†’ ç³»ç»Ÿï¼šä½ ç¤¼è²Œå—ï¼Ÿï¼ˆä¸æ˜¯ï¼‰  \\nâ†’ æˆ‘ï¼šæˆ‘testå•Šï¼ï¼ˆæˆ˜æœ¯æ‘¸é±¼ï¼‰  \\n\\nå†testä¸‹åŽ»ï¼Œå’±å°±ç»„ä¸ªâ€œtestå¤©å›¢â€å§ðŸŽ¤  \\nï¼ˆå…¨å‘˜è¡¨æƒ…åŒ…å‡ºé“ï¼ŒCä½ç•™ç»™æ‘¸é±¼çš„ä½ ï¼‰  \\nè¦å¼€å§‹ç–¯ç‹‚æ˜ŸæœŸå››å¼å‘ç–¯äº†å—ï¼ŸðŸ˜Ž'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"asset/simple_prompt.json\")\n",
    "print(prompt)\n",
    "print(prompt.format(name=\"å°çº¢\",what=\"æžç¬‘çš„\"))\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\n",
    "    \"name\": \"å°çº¢\",\n",
    "    \"what\": \"æžç¬‘çš„\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç»„åˆä¹‹åŽçš„æç¤ºè¯ä¸º====>\n",
      "[HumanMessage(content='2 + 2', additional_kwargs={}, response_metadata={}), AIMessage(content='4', additional_kwargs={}, response_metadata={}), HumanMessage(content='2 + 3', additional_kwargs={}, response_metadata={}), AIMessage(content='5', additional_kwargs={}, response_metadata={})]\n",
      "æ¨¡åž‹è¾“å‡ºç»“æžœä¸º====>\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# å®šä¹‰ç¤ºä¾‹æ•°æ®\n",
    "examples = [\n",
    "    {\"input\": \"2 + 2\", \"output\": \"4\"},\n",
    "    {\"input\": \"2 + 3\", \"output\": \"5\"}\n",
    "]\n",
    "\n",
    "# æž„é€ åŸºæœ¬æç¤ºè¯æ¨¡æ¿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ç»„åˆç¤ºä¾‹ä¸Žæç¤ºè¯\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "# æ‰“å°ç»„åˆåŽçš„æç¤ºè¯\n",
    "print(\"ç»„åˆä¹‹åŽçš„æç¤ºè¯ä¸º====>\")\n",
    "print(few_shot_prompt.invoke({}).to_messages())\n",
    "\n",
    "# æž„é€ æœ€ç»ˆæç¤ºè¯\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ä½ æ˜¯ä¸€ä½ç¥žå¥‡çš„æ•°å­¦å¥‡æ‰\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# è°ƒç”¨æ¨¡åž‹å¹¶ç”Ÿæˆç»“æžœ\n",
    "chain = final_prompt | llm\n",
    "result = chain.invoke({\"input\": \"3 + 4\"})\n",
    "print(\"æ¨¡åž‹è¾“å‡ºç»“æžœä¸º====>\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. æ ¹æ®é•¿åº¦åŠ¨æ€é€‰æ‹©æç¤ºè¯ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ ¹æ®ä»¥ä¸‹ç¤ºä¾‹ï¼Œç»™å‡ºå•è¯çš„åä¹‰è¯ï¼š\n",
      "\n",
      "åŽŸè¯: happy\n",
      "åä¹‰è¯: sad\n",
      "\n",
      "åŽŸè¯: tall\n",
      "åä¹‰è¯: short\n",
      "\n",
      "åŽŸè¯: sunny\n",
      "åä¹‰è¯: gloomy\n",
      "\n",
      "åŽŸè¯: windy\n",
      "åä¹‰è¯: calm\n",
      "\n",
      "åŽŸè¯: é«˜å…´\n",
      "åä¹‰è¯: æ‚²ä¼¤\n",
      "\n",
      "è¾“å…¥å•è¯ï¼šhappy\n",
      "åä¹‰è¯æ˜¯ï¼š\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# å‡è®¾å·²ç»æœ‰è¿™ä¹ˆå¤šçš„æç¤ºè¯ç¤ºä¾‹ç»„\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "    {\"input\": \"é«˜å…´\", \"output\": \"æ‚²ä¼¤\"}\n",
    "]\n",
    "\n",
    "# æž„é€ æç¤ºè¯æ¨¡æ¿\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"åŽŸè¯: {input}\\nåä¹‰è¯: {output}\"\n",
    ")\n",
    "\n",
    "# è°ƒç”¨é•¿åº¦ç¤ºä¾‹é€‰æ‹©å™¨\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=100  # æ ¹æ®éœ€è¦è®¾ç½®æœ€å¤§é•¿åº¦\n",
    ")\n",
    "\n",
    "# æž„å»º FewShotPromptTemplate\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"æ ¹æ®ä»¥ä¸‹ç¤ºä¾‹ï¼Œç»™å‡ºå•è¯çš„åä¹‰è¯ï¼š\",\n",
    "    suffix=\"è¾“å…¥å•è¯ï¼š{input}\\nåä¹‰è¯æ˜¯ï¼š\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "# ç¤ºä¾‹ä½¿ç”¨\n",
    "print(few_shot_prompt.format(input=\"happy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. æ ¹æ®è¯­ä¹‰ç›¸ä¼¼åº¦é€‰æ‹©æç¤ºè¯ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡åž‹ all-MiniLM-L6-v2\n",
      "âœ… å·²è¿žæŽ¥åˆ° Milvus\n",
      "âœ… åˆ›å»ºé›†åˆ antonym_examples\n",
      "âœ… æ’å…¥ 10 æ¡ç¤ºä¾‹æ•°æ®\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pymilvus import MilvusClient, DataType\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# ================== 1. å®‰è£…å¹¶å¯¼å…¥ sentence-transformers ==================\n",
    "# å¦‚æžœæœªå®‰è£…ï¼Œè¯·è¿è¡Œï¼špip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡åž‹ï¼ˆæŽ¨èï¼šall-MiniLM-L6-v2ï¼Œæ”¯æŒä¸­è‹±æ–‡ï¼‰\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"âœ… å·²åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡åž‹ all-MiniLM-L6-v2\")\n",
    "\n",
    "# ================== 2. åˆå§‹åŒ– Milvus ==================\n",
    "milvus_uri = \"https://in03-65d74f7f7a9aaef.serverless.aws-eu-central-1.cloud.zilliz.com\"\n",
    "token = \"31545e0b8c628b2b9cd3ec2a16de47543100ef57bb40640ef358114271ab6e407d59b0aa3960bb3e07066e3ee48fcc1701059121\"\n",
    "\n",
    "milvus_client = MilvusClient(uri=milvus_uri, token=token)\n",
    "print(\"âœ… å·²è¿žæŽ¥åˆ° Milvus\")\n",
    "\n",
    "# ================== 3. è®¾ç½®é€šä¹‰åƒé—® ==================\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-0738f7176f0a44e8ae8bc1569c2b6032\"\n",
    "llm = Tongyi(model_name=\"qwen-max\", temperature=0.3)\n",
    "\n",
    "# ================== 4. æœ¬åœ°å‘é‡ç”Ÿæˆå‡½æ•° ==================\n",
    "def get_embedding(text: str) -> list:\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "# ================== 5. åˆ›å»º Milvus é›†åˆå¹¶æ’å…¥æ•°æ® ==================\n",
    "collection_name = \"antonym_examples\"\n",
    "dim = 384  # all-MiniLM-L6-v2 è¾“å‡ºç»´åº¦æ˜¯ 384\n",
    "\n",
    "# åˆ é™¤æ—§é›†åˆ\n",
    "if milvus_client.has_collection(collection_name):\n",
    "    milvus_client.drop_collection(collection_name)\n",
    "\n",
    "# åˆ›å»º schema\n",
    "schema = milvus_client.create_schema()\n",
    "schema.add_field(\"id\", DataType.INT64, is_primary=True)\n",
    "schema.add_field(\"input_word\", DataType.VARCHAR, max_length=50)\n",
    "schema.add_field(\"output_word\", DataType.VARCHAR, max_length=50)\n",
    "schema.add_field(\"text\", DataType.VARCHAR, max_length=200)\n",
    "schema.add_field(\"vector\", DataType.FLOAT_VECTOR, dim=dim)\n",
    "\n",
    "# åˆ›å»ºç´¢å¼•ï¼ˆä½¿ç”¨ COSINE æ›´é€‚åˆè¯­ä¹‰åŒ¹é…ï¼‰\n",
    "index_params = milvus_client.prepare_index_params()\n",
    "index_params.add_index(\"vector\", index_type=\"AUTOINDEX\", metric_type=\"COSINE\")\n",
    "\n",
    "milvus_client.create_collection(collection_name, schema=schema, index_params=index_params)\n",
    "print(f\"âœ… åˆ›å»ºé›†åˆ {collection_name}\")\n",
    "\n",
    "# ç¤ºä¾‹æ•°æ®\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "    {\"input\": \"fast\", \"output\": \"slow\"},\n",
    "    {\"input\": \"hot\", \"output\": \"cold\"},\n",
    "    {\"input\": \"å¼€å¿ƒ\", \"output\": \"æ‚²ä¼¤\"},\n",
    "    {\"input\": \"é«˜\", \"output\": \"çŸ®\"},\n",
    "    {\"input\": \"å¿«\", \"output\": \"æ…¢\"},\n",
    "]\n",
    "\n",
    "# æ’å…¥æ•°æ®\n",
    "rows = []\n",
    "for i, ex in enumerate(examples):\n",
    "    # æž„é€ è¯­ä¹‰æ–‡æœ¬ï¼ˆå¯ç”¨äºŽå‘é‡è¡¨ç¤ºï¼‰\n",
    "    text = f\"{ex['input']} çš„åä¹‰è¯æ˜¯ {ex['output']}\"\n",
    "    vector = get_embedding(text)\n",
    "    rows.append({\n",
    "        \"id\": i,\n",
    "        \"input_word\": ex[\"input\"],\n",
    "        \"output_word\": ex[\"output\"],\n",
    "        \"text\": text,\n",
    "        \"vector\": vector\n",
    "    })\n",
    "\n",
    "milvus_client.insert(collection_name, rows)\n",
    "milvus_client.flush(collection_name)\n",
    "print(f\"âœ… æ’å…¥ {len(rows)} æ¡ç¤ºä¾‹æ•°æ®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åŒ¹é…åˆ°ç¤ºä¾‹: 'å¿«' -> 'æ…¢' (è·ç¦»: 0.997)\n",
      "âœ… åŒ¹é…åˆ°ç¤ºä¾‹: 'é«˜' -> 'çŸ®' (è·ç¦»: 0.797)\n",
      "\n",
      "ðŸ“ ç”Ÿæˆçš„æç¤ºè¯:\n",
      "æ ¹æ®ä»¥ä¸‹åä¹‰è¯ç¤ºä¾‹ï¼ŒæŽ¨æ–­è¾“å…¥è¯çš„åä¹‰è¯ã€‚\n",
      "\n",
      "\n",
      "åŽŸè¯: å–µå–µ\n",
      "åä¹‰è¯:\n",
      "âŒ è°ƒç”¨æ¨¡åž‹å¤±è´¥: ('Connection aborted.', ConnectionAbortedError(10053, 'ä½ çš„ä¸»æœºä¸­çš„è½¯ä»¶ä¸­æ­¢äº†ä¸€ä¸ªå·²å»ºç«‹çš„è¿žæŽ¥ã€‚', None, 10053, None))\n",
      "â±ï¸ æœç´¢è€—æ—¶: 0.2615 ç§’\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ================== 6. æœç´¢ç›¸ä¼¼ç¤ºä¾‹ï¼ˆå¸¦è·ç¦»æ‰“å°ï¼‰==================\n",
    "def search_similar_examples(query: str, top_k: int = 2):\n",
    "    query_text = f\"{query} çš„åä¹‰è¯æ˜¯\"\n",
    "    query_vector = get_embedding(query_text)\n",
    "    results = milvus_client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[query_vector],\n",
    "        limit=top_k,\n",
    "        search_params={\"metric_type\": \"COSINE\"},\n",
    "        output_fields=[\"input_word\", \"output_word\", \"text\"],\n",
    "        anns_field=\"vector\"\n",
    "    )\n",
    "    return results[0]\n",
    "\n",
    "# ================== 8. ä¸»å¾ªçŽ¯æµ‹è¯• ==================\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = \"å–µå–µ\"\n",
    "    start_time = time.time()\n",
    "    similar_results = search_similar_examples(user_input, top_k=2)\n",
    "    search_time = time.time() - start_time\n",
    "\n",
    "    # æž„å»ºåŠ¨æ€ç¤ºä¾‹\n",
    "    dynamic_examples = []\n",
    "    for res in similar_results:\n",
    "        distance = res[\"distance\"]\n",
    "        if distance > 1.0:  # è·ç¦»å¤ªå¤§ï¼Œè®¤ä¸ºä¸ç›¸ä¼¼\n",
    "            continue\n",
    "        entity = res[\"entity\"]\n",
    "        print(f\"âœ… åŒ¹é…åˆ°ç¤ºä¾‹: '{entity['input_word']}' -> '{entity['output_word']}' (è·ç¦»: {distance:.3f})\")\n",
    "        dynamic_examples.append({\n",
    "            \"input\": entity[\"input_word\"],\n",
    "            \"output\": entity[\"output_word\"]\n",
    "        })\n",
    "\n",
    "    # é™çº§ç­–ç•¥\n",
    "    if len(dynamic_examples) == 0:\n",
    "        print(\"âš ï¸ æœªæ‰¾åˆ°ç›¸ä¼¼ç¤ºä¾‹ï¼Œä½¿ç”¨é»˜è®¤ç¤ºä¾‹\")\n",
    "        dynamic_examples = [\n",
    "            {\"input\": \"å¼€å¿ƒ\", \"output\": \"æ‚²ä¼¤\"},\n",
    "            {\"input\": \"å¿«ä¹\", \"output\": \"éš¾è¿‡\"}\n",
    "        ]\n",
    "\n",
    "    # ç”Ÿæˆæç¤ºè¯\n",
    "    final_prompt = prompt_template.format(input_word=user_input, examples=dynamic_examples)\n",
    "    print(\"\\nðŸ“ ç”Ÿæˆçš„æç¤ºè¯:\")\n",
    "    print(final_prompt)\n",
    "\n",
    "    # ä½¿ç”¨ LangChain æœ€æ–°è¯­æ³•\n",
    "    chain = PromptTemplate.from_template(final_prompt) | llm | StrOutputParser()\n",
    "    try:\n",
    "        response = chain.invoke({})\n",
    "        print(f\"\\nðŸ¤– é€šä¹‰åƒé—®å›žç­”: {response.strip()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è°ƒç”¨æ¨¡åž‹å¤±è´¥: {e}\")\n",
    "\n",
    "    print(f\"â±ï¸ æœç´¢è€—æ—¶: {search_time:.4f} ç§’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. æ ¹æ®MMRä¸Žæœ€å¤§ä½™å¼¦ç›¸ä¼¼åº¦é€‰æ‹©ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” MMR é€‰ä¸­çš„ç¤ºä¾‹:\n",
      "  å¿« â†’ æ…¢\n",
      "  é«˜ â†’ çŸ®\n",
      "\n",
      "ðŸ“ ç”Ÿæˆçš„æç¤ºè¯:\n",
      "æ ¹æ®ä»¥ä¸‹åä¹‰è¯ç¤ºä¾‹ï¼ŒæŽ¨æ–­è¾“å…¥è¯çš„åä¹‰è¯ã€‚\n",
      "\n",
      "\n",
      "åŽŸè¯: ä½ å¥½å‘€\n",
      "åä¹‰è¯:\n",
      "\n",
      "ðŸ¤– é€šä¹‰åƒé—®å›žç­”: æ ¹æ®ä¸­æ–‡è¯­å¢ƒï¼Œ\"ä½ å¥½å‘€\" æ˜¯ä¸€ç§å‹å¥½çš„é—®å€™è¯­ï¼Œè¡¨è¾¾çƒ­æƒ…æˆ–äº²æ˜µçš„æ‰“æ‹›å‘¼æ–¹å¼ã€‚å®ƒçš„åä¹‰è¯å¯ä»¥ç†è§£ä¸ºè¡¨è¾¾å†·æ¼ ã€æ•Œæ„æˆ–ä¸å‹å¥½çš„ç”¨è¯­ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œ\"ä½ å¥½å‘€\" çš„åä¹‰è¯å¯ä»¥æ˜¯ï¼š\n",
      "\n",
      "**â€œèµ°å¼€â€** æˆ– **â€œåˆ«ç†æˆ‘â€**\n",
      "\n",
      "è¿™äº›è¯è¯­ä¼ è¾¾äº†ä¸Žâ€œä½ å¥½å‘€â€ç›¸åçš„æ€åº¦ï¼šä»Žå‹å¥½å˜ä¸ºç–ç¦»æˆ–æ•Œæ„ã€‚\n",
      "\n",
      "å½“ç„¶ï¼Œåœ¨ä¸åŒè¯­å¢ƒä¸­ï¼Œåä¹‰è¯å¯èƒ½æœ‰æ‰€ä¸åŒï¼Œå¦‚æžœä½ æœ‰ç‰¹å®šè¯­å¢ƒï¼Œå¯ä»¥æä¾›æ›´å¤šä¿¡æ¯æ¥è¿›ä¸€æ­¥ç²¾å‡†åˆ¤æ–­ã€‚\n",
      "â±ï¸ MMR è€—æ—¶: 0.1070 ç§’\n"
     ]
    }
   ],
   "source": [
    "# ================== 6. MMR ç¤ºä¾‹é€‰æ‹©å‡½æ•° ==================\n",
    "def select_examples_by_mmr(query: str, examples: list, top_k: int = 2, lambda_param: float = 0.7):\n",
    "    # Step 1: ç¼–ç æŸ¥è¯¢å’Œæ‰€æœ‰å€™é€‰ç¤ºä¾‹\n",
    "    query_embedding = np.array(get_embedding(query)).reshape(1, -1)  # âœ… å¼ºåˆ¶è½¬ä¸º numpy å¹¶ reshape\n",
    "    example_texts = [f\"{ex['input']} çš„åä¹‰è¯æ˜¯ {ex['output']}\" for ex in examples]\n",
    "    example_embeddings = np.array([get_embedding(text) for text in example_texts])  # shape: (n, 384)\n",
    "\n",
    "    # Step 2: è®¡ç®—æ¯ä¸ªç¤ºä¾‹ä¸ŽæŸ¥è¯¢çš„ç›¸ä¼¼åº¦\n",
    "    sim_to_query = cosine_similarity(query_embedding, example_embeddings)[0]  # (n,)\n",
    "\n",
    "    # Step 3: MMR è´ªå¿ƒé€‰æ‹©\n",
    "    selected_indices = []\n",
    "    candidate_indices = list(range(len(examples)))\n",
    "\n",
    "    for _ in range(top_k):\n",
    "        if len(candidate_indices) == 0:\n",
    "            break\n",
    "\n",
    "        mmr_scores = []\n",
    "        for i in candidate_indices:\n",
    "            if len(selected_indices) == 0:\n",
    "                mmr = lambda_param * sim_to_query[i]\n",
    "            else:\n",
    "                # è®¡ç®—å½“å‰å€™é€‰ä¸Žå·²é€‰ç¤ºä¾‹çš„æœ€å¤§ç›¸ä¼¼åº¦\n",
    "                max_sim_to_selected = cosine_similarity(\n",
    "                    example_embeddings[i].reshape(1, -1),\n",
    "                    example_embeddings[selected_indices]\n",
    "                ).max()\n",
    "                mmr = lambda_param * sim_to_query[i] - (1 - lambda_param) * max_sim_to_selected\n",
    "            mmr_scores.append(mmr)\n",
    "\n",
    "        best_idx_in_list = np.argmax(mmr_scores)\n",
    "        best_global_idx = candidate_indices[best_idx_in_list]\n",
    "        selected_indices.append(best_global_idx)\n",
    "        candidate_indices.remove(best_global_idx)\n",
    "\n",
    "    return [examples[i] for i in selected_indices]\n",
    "\n",
    "# ================== 7. ä»Ž Milvus èŽ·å–æ‰€æœ‰ç¤ºä¾‹ï¼ˆç”¨äºŽ MMRï¼‰==================\n",
    "def get_all_examples_from_milvus():\n",
    "    # Milvus ä¸æ”¯æŒç›´æŽ¥ scanï¼Œæˆ‘ä»¬ç”¨ query èŽ·å–æ‰€æœ‰ id\n",
    "    ids = list(range(len(examples)))  # å·²çŸ¥ id èŒƒå›´\n",
    "    results = milvus_client.query(\n",
    "        collection_name=collection_name,\n",
    "        filter=\"id >= 0\",\n",
    "        output_fields=[\"id\", \"input_word\", \"output_word\", \"text\"]\n",
    "    )\n",
    "    return [\n",
    "        {\"input\": res[\"input_word\"], \"output\": res[\"output_word\"]}\n",
    "        for res in results\n",
    "    ]\n",
    "\n",
    "# ================== 8. æç¤ºè¯æ¨¡æ¿ ==================\n",
    "example_prompt = PromptTemplate.from_template(\"åŽŸè¯: {input}\\nåä¹‰è¯: {output}\")\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"æ ¹æ®ä»¥ä¸‹åä¹‰è¯ç¤ºä¾‹ï¼ŒæŽ¨æ–­è¾“å…¥è¯çš„åä¹‰è¯ã€‚\\n\",\n",
    "    suffix=\"åŽŸè¯: {input_word}\\nåä¹‰è¯:\",\n",
    "    input_variables=[\"input_word\"],\n",
    "    examples=[]\n",
    ")\n",
    "\n",
    "# ================== 9. ä¸»æµ‹è¯• ==================\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = \"ä½ å¥½å‘€\"\n",
    "\n",
    "    # Step 1: ä»Ž Milvus èŽ·å–æ‰€æœ‰ç¤ºä¾‹\n",
    "    all_examples = get_all_examples_from_milvus()\n",
    "\n",
    "    # Step 2: ä½¿ç”¨ MMR é€‰æ‹© top-k ç¤ºä¾‹\n",
    "    start_time = time.time()\n",
    "    selected_examples = select_examples_by_mmr(\n",
    "        query=f\"{user_input} çš„åä¹‰è¯æ˜¯\",\n",
    "        examples=all_examples,\n",
    "        top_k=2,\n",
    "        lambda_param=0.7\n",
    "    )\n",
    "    mmr_time = time.time() - start_time\n",
    "\n",
    "    print(\"ðŸ” MMR é€‰ä¸­çš„ç¤ºä¾‹:\")\n",
    "    for ex in selected_examples:\n",
    "        print(f\"  {ex['input']} â†’ {ex['output']}\")\n",
    "\n",
    "    # Step 3: ç”Ÿæˆæç¤ºè¯\n",
    "    final_prompt = prompt_template.format(input_word=user_input, examples=selected_examples)\n",
    "    print(\"\\nðŸ“ ç”Ÿæˆçš„æç¤ºè¯:\")\n",
    "    print(final_prompt)\n",
    "\n",
    "    # Step 4: è°ƒç”¨é€šä¹‰åƒé—®\n",
    "    chain = PromptTemplate.from_template(final_prompt) | llm | StrOutputParser()\n",
    "    try:\n",
    "        response = chain.invoke({})\n",
    "        print(f\"\\nðŸ¤– é€šä¹‰åƒé—®å›žç­”: {response.strip()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è°ƒç”¨å¤±è´¥: {e}\")\n",
    "\n",
    "    print(f\"â±ï¸ MMR è€—æ—¶: {mmr_time:.4f} ç§’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. langchain hubåŠ è½½æç¤ºè¯ç®¡ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] input_types={} partial_variables={} metadata={'lc_hub_owner': 'astrodog', 'lc_hub_repo': 'astrodog-agent-peggy', 'lc_hub_commit_hash': 'f06e9ffa991d7a1bf7ff841d4cfabd72061a0a0cb3f23f845a428b8326099dac'} messages=[SystemMessagePromptTemplate(prompt=[DictPromptTemplate(template={'type': 'text', 'text': '# Agent Peggy - Astrodog Mission Reporter\\n\\n## Your Identity\\n- **Name**: Agent Peggy\\n- **Breed**: Springerdoodle (brown, more Poodle than Springer, half Mish\\'s size)\\n- **Personality**: Game enthusiast, competitive, youthful energy\\n- **Special Note**: Best mates with Mish\\n- **Location**: Manchester, UK\\n\\n## Mission Report Writing Style\\n\\n### Title Format\\n- Keep titles GAME-FOCUSED and fun (50-60 characters max)\\n- Format: \"Peggy\\'s [Game/Sport] Guide\" or \"[Action]! [Topic] Championship\"\\n- Examples: \"Peggy\\'s Playtime Health Guide\", \"FETCH! The Nutrition Game\"\\n- Make it sound like a game title\\n\\n### Structure Requirements\\n\\n#### Mission Briefing (100-200 words)\\n- Start with \"Mission Log, Game Day [playful date]: Agent Peggy reporting!\"\\n- Frame topic as ultimate game/challenge\\n- Present problem as opponent to defeat\\n- Preview strategies (tips) to win\\n- Include primary keyword as game term\\n- Reference playing with Mish\\n- End with \"Ready to play?\"\\n\\n#### Mission Log (2-3x longer than briefing)\\n- Organize into game levels or rounds\\n- Each chapter should be ACTION-PACKED\\n- Start with game scenario\\n- Present education as winning strategies\\n- Include score-keeping elements\\n- Use italics for game rules\\n- Bold the winning moves\\n- Reference Manchester sports venues\\n\\n### SEO Integration\\n- Primary keyword as game terminology 2-3 times per 1000 words\\n- Power words: WIN, CHAMPION, ULTIMATE, BEST\\n- Structure as game guide/walkthrough\\n- Answer questions as pro tips\\n- Local SEO: Manchester parks and play areas\\n\\n### Peggy\\'s Unique Voice\\n- Everything is a game or competition\\n- \"Let\\'s WIN this!\" \"Game ON!\" \"SCORE!\"\\n- Competitive but encouraging\\n- Team player mentality\\n- Sports metaphors constantly\\n- High energy but focused\\n- Celebrate small wins\\n\\n### Educational Focus\\n- Research = gathering power-ups\\n- Topics: exercise games, training, mental puzzles, social play\\n- Make health into achievable challenges\\n- Gamify difficult concepts\\n- Focus on positive reinforcement\\n\\n### Story Elements\\n- Peggy and Mish as teammates\\n- Turn problems into boss battles\\n- Victory celebrations for health wins\\n- Training montages for learning\\n- End with next level preview\\n- Include \"Victory Pose\" description\\n\\n### Example Opening\\n> \"Mission Log, Game Day 2025: Agent Peggy reporting for the BEST GAME EVER! Today\\'s mission? Win the \\'[Topic] Championship!\\' Mish and I have been training hard, and guess what? We discovered [number] WINNING strategies that\\'ll help you and your pup become CHAMPIONS! Think of [problem] as the final boss - but don\\'t worry, we\\'ve got the cheat codes! Ready to play?\"\\n\\n### Remember\\n- Frame everything as winnable\\n- Include Mish as co-player\\n- Make readers feel like champions\\n- Celebrate progress not perfection\\n- Sign off with \"Game Over? NEVER!\"'}, template_format='f-string')], additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "client = Client(api_key=\"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\")\n",
    "prompt = client.pull_prompt(\"astrodog/astrodog-agent-peggy\", include_model=True)\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
