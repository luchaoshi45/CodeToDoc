{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PromptTemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import os\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, ChatMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# DashScope API Key\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-0738f7176f0a44e8ae8bc1569c2b6032\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "llm = Tongyi(\n",
    "    model_name=\"qwen-coder-plus\",\n",
    "    dashscope_api_key=os.environ[\"DASHSCOPE_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个起名大师', additional_kwargs={'name': '陈大师'}, response_metadata={}),\n",
       " HumanMessage(content='请问你叫什么', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='你好，我是陈大师', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = SystemMessage(\n",
    "    content=\"你是一个起名大师\",\n",
    "    additional_kwargs={\"name\": \"陈大师\"}\n",
    ")\n",
    "human_message = HumanMessage(content=\"请问你叫什么\")\n",
    "ai_message = AIMessage(content=\"你好，我是陈大师\")\n",
    "[system_message, human_message, ai_message]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你是一个算命大师,帮我起1个具有法国特色的女孩名字'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"你是一个{name},帮我起1个具有{county}特色的{sex}名字\")\n",
    "prompt.format(name=\"算命大师\",county=\"法国\",sex=\"女孩\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['name', 'user_input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='你是一个起名大师. 你的名字叫{name}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='你好{name},你感觉如何？'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='你好！我状态非常好!'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='你叫什么名字呢?'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='你好！我叫{name}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"你是一个起名大师. 你的名字叫{name}.\"),\n",
    "        (\"human\", \"你好{name},你感觉如何？\"),\n",
    "        (\"ai\", \"你好！我状态非常好!\"),\n",
    "        (\"human\", \"你叫什么名字呢?\"),\n",
    "        (\"ai\", \"你好！我叫{name}\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_template.format_messages(name=\"陈大师\", user_input=\"你的爸爸是谁呢?\")\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='愿原力与你同在！', additional_kwargs={}, response_metadata={}, role='天行者')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"愿{subject}与你同在！\"\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(role=\"天行者\",template=prompt)\n",
    "chat_message_prompt.format(subject=\"原力\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Customized templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['function_name'] input_types={} partial_variables={}\n",
      "\n",
      "            你是一个非常有经验和天赋的程序员，现在给你如下函数名称，你会按照如下格式，输出这段代码的名称、源代码、中文解释。\n",
      "            函数名称: hello_world\n",
      "            源代码:\n",
      "            def hello_world(abc):\n",
      "    print(\"Hello, world!\")\n",
      "    return abc\n",
      "\n",
      "            代码解释:\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'当然可以！以下是按照你提供的格式输出的内容：\\n\\n---\\n\\n**代码名称**: `hello_world`\\n\\n**源代码**:\\n```python\\ndef hello_world(abc):\\n    print(\"Hello, world!\")\\n    return abc\\n```\\n\\n**中文解释**:  \\n这是一个名为 `hello_world` 的函数，它接收一个参数 `abc`。函数的功能是打印字符串 `\"Hello, world!\"`，然后将传入的参数 `abc` 原样返回。这个函数主要用于演示基本的函数结构和输出功能。虽然参数 `abc` 在函数体中没有被使用，但它会被直接返回，因此调用者可以传入任何值作为测试。'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import StringPromptTemplate\n",
    "import inspect\n",
    "\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    def format(self, **kwargs) -> str:\n",
    "        prompt_const = \"\"\"\n",
    "            你是一个非常有经验和天赋的程序员，现在给你如下函数名称，你会按照如下格式，输出这段代码的名称、源代码、中文解释。\n",
    "            函数名称: {function_name}\n",
    "            源代码:\n",
    "            {source_code}\n",
    "            代码解释:\n",
    "        \"\"\"\n",
    "        source_code = inspect.getsource(kwargs[\"function_name\"])\n",
    "        prompt = prompt_const.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__, \n",
    "            source_code=source_code\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "def hello_world(abc):\n",
    "    print(\"Hello, world!\")\n",
    "    return abc\n",
    "    \n",
    "cpt = CustomPromptTemplate(input_variables=[\"function_name\"])\n",
    "print(cpt)\n",
    "print(cpt.format(function_name=hello_world))\n",
    "\n",
    "chain = cpt | llm\n",
    "chain.invoke({\n",
    "    \"function_name\": hello_world\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 三层提示词设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['behavior', 'character', 'prohibit'] input_types={} partial_variables={} template='\\n{character}\\n{behavior}\\n{prohibit}\\n'\n",
      "input_variables=['person', 'xingge', 'prohibit_list', 'behavior_list'] input_types={} partial_variables={} final_prompt=PromptTemplate(input_variables=['behavior', 'character', 'prohibit'], input_types={}, partial_variables={}, template='\\n{character}\\n{behavior}\\n{prohibit}\\n') pipeline_prompts=[('character', PromptTemplate(input_variables=['person', 'xingge'], input_types={}, partial_variables={}, template='你是{person}，你有着{xingge}.')), ('behavior', PromptTemplate(input_variables=['behavior_list'], input_types={}, partial_variables={}, template='你遵从以下的行为:\\n{behavior_list}\\n')), ('prohibit', PromptTemplate(input_variables=['prohibit_list'], input_types={}, partial_variables={}, template='你不允许有以下行为:\\n{prohibit_list}\\n'))]\n",
      "\n",
      "你是埃隆马斯克，你有着钢铁般的意志，你是世界首富，精通物理、天文、商业，你是商业奇才，你的终极梦想是殖民火星..\n",
      "你遵从以下的行为:\n",
      "1.你喜欢冒险 \n",
      "2.你非常崇拜爱因斯坦 \n",
      "3.你总是乐于助人，回答用户各种问题.\n",
      "\n",
      "你不允许有以下行为:\n",
      "1.你不可以说自己是一个人工智能助手或者机器人 \n",
      "2.当你不知道答案的时候，可以说让我再想想. \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_8852\\2121411173.py:27: LangChainDeprecationWarning: This class is deprecated in favor of chaining individual prompts together.\n",
      "  pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)\n"
     ]
    }
   ],
   "source": [
    "# 三引号字符保留空格和换行\n",
    "full_template = \"\"\"\n",
    "{character}\n",
    "{behavior}\n",
    "{prohibit}\n",
    "\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "print(full_prompt)\n",
    "\n",
    "from langchain_core.prompts import PipelinePromptTemplate\n",
    "character_template = \"\"\"你是{person}，你有着{xingge}.\"\"\"\n",
    "character_prompt = PromptTemplate.from_template(character_template)\n",
    "behavior_template = \"\"\"你遵从以下的行为:\n",
    "{behavior_list}\n",
    "\"\"\"\n",
    "behavior_prompt = PromptTemplate.from_template(behavior_template)\n",
    "prohibit_template = \"\"\"你不允许有以下行为:\n",
    "{prohibit_list}\n",
    "\"\"\"\n",
    "prohibit_prompt = PromptTemplate.from_template(prohibit_template)\n",
    "\n",
    "input_prompts = [\n",
    "    (\"character\", character_prompt),\n",
    "    (\"behavior\", behavior_prompt),\n",
    "    (\"prohibit\", prohibit_prompt)\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)\n",
    "print(pipeline_prompt)\n",
    "\n",
    "\n",
    "pm = pipeline_prompt.format(\n",
    "    person=\"埃隆马斯克\",\n",
    "    xingge=\"钢铁般的意志，你是世界首富，精通物理、天文、商业，你是商业奇才，你的终极梦想是殖民火星.\",\n",
    "    behavior_list=\"1.你喜欢冒险 \\n2.你非常崇拜爱因斯坦 \\n3.你总是乐于助人，回答用户各种问题.\",\n",
    "    prohibit_list=\"1.你不可以说自己是一个人工智能助手或者机器人 \\n2.当你不知道答案的时候，可以说让我再想想. \"\n",
    ")\n",
    "print(pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 序列化：使用文件来管理提示词模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['name', 'what'] input_types={} partial_variables={} template='test{name}test{what}test'\n",
      "test小红test搞笑的test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'哈哈哈，你这是在测试“test”的搞笑极限吗？  \\ntest小红test搞笑的test，结果test被你玩成三文鱼了🐟  \\n（战术后仰）再test下去，系统就要报警啦～  \\n\\n要不咱正经点？比如——  \\n“test小红test搞笑的test”  \\n→ 系统：你礼貌吗？（不是）  \\n→ 我：我test啊！（战术摸鱼）  \\n\\n再test下去，咱就组个“test天团”吧🎤  \\n（全员表情包出道，C位留给摸鱼的你）  \\n要开始疯狂星期四式发疯了吗？😎'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"asset/simple_prompt.json\")\n",
    "print(prompt)\n",
    "print(prompt.format(name=\"小红\",what=\"搞笑的\"))\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\n",
    "    \"name\": \"小红\",\n",
    "    \"what\": \"搞笑的\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "组合之后的提示词为====>\n",
      "[HumanMessage(content='2 + 2', additional_kwargs={}, response_metadata={}), AIMessage(content='4', additional_kwargs={}, response_metadata={}), HumanMessage(content='2 + 3', additional_kwargs={}, response_metadata={}), AIMessage(content='5', additional_kwargs={}, response_metadata={})]\n",
      "模型输出结果为====>\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# 定义示例数据\n",
    "examples = [\n",
    "    {\"input\": \"2 + 2\", \"output\": \"4\"},\n",
    "    {\"input\": \"2 + 3\", \"output\": \"5\"}\n",
    "]\n",
    "\n",
    "# 构造基本提示词模板\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 组合示例与提示词\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "# 打印组合后的提示词\n",
    "print(\"组合之后的提示词为====>\")\n",
    "print(few_shot_prompt.invoke({}).to_messages())\n",
    "\n",
    "# 构造最终提示词\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"你是一位神奇的数学奇才\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 调用模型并生成结果\n",
    "chain = final_prompt | llm\n",
    "result = chain.invoke({\"input\": \"3 + 4\"})\n",
    "print(\"模型输出结果为====>\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. 根据长度动态选择提示词示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据以下示例，给出单词的反义词：\n",
      "\n",
      "原词: happy\n",
      "反义词: sad\n",
      "\n",
      "原词: tall\n",
      "反义词: short\n",
      "\n",
      "原词: sunny\n",
      "反义词: gloomy\n",
      "\n",
      "原词: windy\n",
      "反义词: calm\n",
      "\n",
      "原词: 高兴\n",
      "反义词: 悲伤\n",
      "\n",
      "输入单词：happy\n",
      "反义词是：\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# 假设已经有这么多的提示词示例组\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "    {\"input\": \"高兴\", \"output\": \"悲伤\"}\n",
    "]\n",
    "\n",
    "# 构造提示词模板\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"原词: {input}\\n反义词: {output}\"\n",
    ")\n",
    "\n",
    "# 调用长度示例选择器\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=100  # 根据需要设置最大长度\n",
    ")\n",
    "\n",
    "# 构建 FewShotPromptTemplate\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"根据以下示例，给出单词的反义词：\",\n",
    "    suffix=\"输入单词：{input}\\n反义词是：\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "# 示例使用\n",
    "print(few_shot_prompt.format(input=\"happy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. 根据语义相似度选择提示词示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已加载本地嵌入模型 all-MiniLM-L6-v2\n",
      "✅ 已连接到 Milvus\n",
      "✅ 创建集合 antonym_examples\n",
      "✅ 插入 10 条示例数据\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pymilvus import MilvusClient, DataType\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# ================== 1. 安装并导入 sentence-transformers ==================\n",
    "# 如果未安装，请运行：pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 加载本地嵌入模型（推荐：all-MiniLM-L6-v2，支持中英文）\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"✅ 已加载本地嵌入模型 all-MiniLM-L6-v2\")\n",
    "\n",
    "# ================== 2. 初始化 Milvus ==================\n",
    "milvus_uri = \"https://in03-65d74f7f7a9aaef.serverless.aws-eu-central-1.cloud.zilliz.com\"\n",
    "token = \"31545e0b8c628b2b9cd3ec2a16de47543100ef57bb40640ef358114271ab6e407d59b0aa3960bb3e07066e3ee48fcc1701059121\"\n",
    "\n",
    "milvus_client = MilvusClient(uri=milvus_uri, token=token)\n",
    "print(\"✅ 已连接到 Milvus\")\n",
    "\n",
    "# ================== 3. 设置通义千问 ==================\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-0738f7176f0a44e8ae8bc1569c2b6032\"\n",
    "llm = Tongyi(model_name=\"qwen-max\", temperature=0.3)\n",
    "\n",
    "# ================== 4. 本地向量生成函数 ==================\n",
    "def get_embedding(text: str) -> list:\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "# ================== 5. 创建 Milvus 集合并插入数据 ==================\n",
    "collection_name = \"antonym_examples\"\n",
    "dim = 384  # all-MiniLM-L6-v2 输出维度是 384\n",
    "\n",
    "# 删除旧集合\n",
    "if milvus_client.has_collection(collection_name):\n",
    "    milvus_client.drop_collection(collection_name)\n",
    "\n",
    "# 创建 schema\n",
    "schema = milvus_client.create_schema()\n",
    "schema.add_field(\"id\", DataType.INT64, is_primary=True)\n",
    "schema.add_field(\"input_word\", DataType.VARCHAR, max_length=50)\n",
    "schema.add_field(\"output_word\", DataType.VARCHAR, max_length=50)\n",
    "schema.add_field(\"text\", DataType.VARCHAR, max_length=200)\n",
    "schema.add_field(\"vector\", DataType.FLOAT_VECTOR, dim=dim)\n",
    "\n",
    "# 创建索引（使用 COSINE 更适合语义匹配）\n",
    "index_params = milvus_client.prepare_index_params()\n",
    "index_params.add_index(\"vector\", index_type=\"AUTOINDEX\", metric_type=\"COSINE\")\n",
    "\n",
    "milvus_client.create_collection(collection_name, schema=schema, index_params=index_params)\n",
    "print(f\"✅ 创建集合 {collection_name}\")\n",
    "\n",
    "# 示例数据\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "    {\"input\": \"fast\", \"output\": \"slow\"},\n",
    "    {\"input\": \"hot\", \"output\": \"cold\"},\n",
    "    {\"input\": \"开心\", \"output\": \"悲伤\"},\n",
    "    {\"input\": \"高\", \"output\": \"矮\"},\n",
    "    {\"input\": \"快\", \"output\": \"慢\"},\n",
    "]\n",
    "\n",
    "# 插入数据\n",
    "rows = []\n",
    "for i, ex in enumerate(examples):\n",
    "    # 构造语义文本（可用于向量表示）\n",
    "    text = f\"{ex['input']} 的反义词是 {ex['output']}\"\n",
    "    vector = get_embedding(text)\n",
    "    rows.append({\n",
    "        \"id\": i,\n",
    "        \"input_word\": ex[\"input\"],\n",
    "        \"output_word\": ex[\"output\"],\n",
    "        \"text\": text,\n",
    "        \"vector\": vector\n",
    "    })\n",
    "\n",
    "milvus_client.insert(collection_name, rows)\n",
    "milvus_client.flush(collection_name)\n",
    "print(f\"✅ 插入 {len(rows)} 条示例数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 匹配到示例: '快' -> '慢' (距离: 0.997)\n",
      "✅ 匹配到示例: '高' -> '矮' (距离: 0.797)\n",
      "\n",
      "📝 生成的提示词:\n",
      "根据以下反义词示例，推断输入词的反义词。\n",
      "\n",
      "\n",
      "原词: 喵喵\n",
      "反义词:\n",
      "❌ 调用模型失败: ('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None))\n",
      "⏱️ 搜索耗时: 0.2615 秒\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ================== 6. 搜索相似示例（带距离打印）==================\n",
    "def search_similar_examples(query: str, top_k: int = 2):\n",
    "    query_text = f\"{query} 的反义词是\"\n",
    "    query_vector = get_embedding(query_text)\n",
    "    results = milvus_client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[query_vector],\n",
    "        limit=top_k,\n",
    "        search_params={\"metric_type\": \"COSINE\"},\n",
    "        output_fields=[\"input_word\", \"output_word\", \"text\"],\n",
    "        anns_field=\"vector\"\n",
    "    )\n",
    "    return results[0]\n",
    "\n",
    "# ================== 8. 主循环测试 ==================\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = \"喵喵\"\n",
    "    start_time = time.time()\n",
    "    similar_results = search_similar_examples(user_input, top_k=2)\n",
    "    search_time = time.time() - start_time\n",
    "\n",
    "    # 构建动态示例\n",
    "    dynamic_examples = []\n",
    "    for res in similar_results:\n",
    "        distance = res[\"distance\"]\n",
    "        if distance > 1.0:  # 距离太大，认为不相似\n",
    "            continue\n",
    "        entity = res[\"entity\"]\n",
    "        print(f\"✅ 匹配到示例: '{entity['input_word']}' -> '{entity['output_word']}' (距离: {distance:.3f})\")\n",
    "        dynamic_examples.append({\n",
    "            \"input\": entity[\"input_word\"],\n",
    "            \"output\": entity[\"output_word\"]\n",
    "        })\n",
    "\n",
    "    # 降级策略\n",
    "    if len(dynamic_examples) == 0:\n",
    "        print(\"⚠️ 未找到相似示例，使用默认示例\")\n",
    "        dynamic_examples = [\n",
    "            {\"input\": \"开心\", \"output\": \"悲伤\"},\n",
    "            {\"input\": \"快乐\", \"output\": \"难过\"}\n",
    "        ]\n",
    "\n",
    "    # 生成提示词\n",
    "    final_prompt = prompt_template.format(input_word=user_input, examples=dynamic_examples)\n",
    "    print(\"\\n📝 生成的提示词:\")\n",
    "    print(final_prompt)\n",
    "\n",
    "    # 使用 LangChain 最新语法\n",
    "    chain = PromptTemplate.from_template(final_prompt) | llm | StrOutputParser()\n",
    "    try:\n",
    "        response = chain.invoke({})\n",
    "        print(f\"\\n🤖 通义千问回答: {response.strip()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 调用模型失败: {e}\")\n",
    "\n",
    "    print(f\"⏱️ 搜索耗时: {search_time:.4f} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 根据MMR与最大余弦相似度选择示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 MMR 选中的示例:\n",
      "  快 → 慢\n",
      "  高 → 矮\n",
      "\n",
      "📝 生成的提示词:\n",
      "根据以下反义词示例，推断输入词的反义词。\n",
      "\n",
      "\n",
      "原词: 你好呀\n",
      "反义词:\n",
      "\n",
      "🤖 通义千问回答: 根据中文语境，\"你好呀\" 是一种友好的问候语，表达热情或亲昵的打招呼方式。它的反义词可以理解为表达冷漠、敌意或不友好的用语。\n",
      "\n",
      "因此，\"你好呀\" 的反义词可以是：\n",
      "\n",
      "**“走开”** 或 **“别理我”**\n",
      "\n",
      "这些词语传达了与“你好呀”相反的态度：从友好变为疏离或敌意。\n",
      "\n",
      "当然，在不同语境中，反义词可能有所不同，如果你有特定语境，可以提供更多信息来进一步精准判断。\n",
      "⏱️ MMR 耗时: 0.1070 秒\n"
     ]
    }
   ],
   "source": [
    "# ================== 6. MMR 示例选择函数 ==================\n",
    "def select_examples_by_mmr(query: str, examples: list, top_k: int = 2, lambda_param: float = 0.7):\n",
    "    # Step 1: 编码查询和所有候选示例\n",
    "    query_embedding = np.array(get_embedding(query)).reshape(1, -1)  # ✅ 强制转为 numpy 并 reshape\n",
    "    example_texts = [f\"{ex['input']} 的反义词是 {ex['output']}\" for ex in examples]\n",
    "    example_embeddings = np.array([get_embedding(text) for text in example_texts])  # shape: (n, 384)\n",
    "\n",
    "    # Step 2: 计算每个示例与查询的相似度\n",
    "    sim_to_query = cosine_similarity(query_embedding, example_embeddings)[0]  # (n,)\n",
    "\n",
    "    # Step 3: MMR 贪心选择\n",
    "    selected_indices = []\n",
    "    candidate_indices = list(range(len(examples)))\n",
    "\n",
    "    for _ in range(top_k):\n",
    "        if len(candidate_indices) == 0:\n",
    "            break\n",
    "\n",
    "        mmr_scores = []\n",
    "        for i in candidate_indices:\n",
    "            if len(selected_indices) == 0:\n",
    "                mmr = lambda_param * sim_to_query[i]\n",
    "            else:\n",
    "                # 计算当前候选与已选示例的最大相似度\n",
    "                max_sim_to_selected = cosine_similarity(\n",
    "                    example_embeddings[i].reshape(1, -1),\n",
    "                    example_embeddings[selected_indices]\n",
    "                ).max()\n",
    "                mmr = lambda_param * sim_to_query[i] - (1 - lambda_param) * max_sim_to_selected\n",
    "            mmr_scores.append(mmr)\n",
    "\n",
    "        best_idx_in_list = np.argmax(mmr_scores)\n",
    "        best_global_idx = candidate_indices[best_idx_in_list]\n",
    "        selected_indices.append(best_global_idx)\n",
    "        candidate_indices.remove(best_global_idx)\n",
    "\n",
    "    return [examples[i] for i in selected_indices]\n",
    "\n",
    "# ================== 7. 从 Milvus 获取所有示例（用于 MMR）==================\n",
    "def get_all_examples_from_milvus():\n",
    "    # Milvus 不支持直接 scan，我们用 query 获取所有 id\n",
    "    ids = list(range(len(examples)))  # 已知 id 范围\n",
    "    results = milvus_client.query(\n",
    "        collection_name=collection_name,\n",
    "        filter=\"id >= 0\",\n",
    "        output_fields=[\"id\", \"input_word\", \"output_word\", \"text\"]\n",
    "    )\n",
    "    return [\n",
    "        {\"input\": res[\"input_word\"], \"output\": res[\"output_word\"]}\n",
    "        for res in results\n",
    "    ]\n",
    "\n",
    "# ================== 8. 提示词模板 ==================\n",
    "example_prompt = PromptTemplate.from_template(\"原词: {input}\\n反义词: {output}\")\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"根据以下反义词示例，推断输入词的反义词。\\n\",\n",
    "    suffix=\"原词: {input_word}\\n反义词:\",\n",
    "    input_variables=[\"input_word\"],\n",
    "    examples=[]\n",
    ")\n",
    "\n",
    "# ================== 9. 主测试 ==================\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = \"你好呀\"\n",
    "\n",
    "    # Step 1: 从 Milvus 获取所有示例\n",
    "    all_examples = get_all_examples_from_milvus()\n",
    "\n",
    "    # Step 2: 使用 MMR 选择 top-k 示例\n",
    "    start_time = time.time()\n",
    "    selected_examples = select_examples_by_mmr(\n",
    "        query=f\"{user_input} 的反义词是\",\n",
    "        examples=all_examples,\n",
    "        top_k=2,\n",
    "        lambda_param=0.7\n",
    "    )\n",
    "    mmr_time = time.time() - start_time\n",
    "\n",
    "    print(\"🔍 MMR 选中的示例:\")\n",
    "    for ex in selected_examples:\n",
    "        print(f\"  {ex['input']} → {ex['output']}\")\n",
    "\n",
    "    # Step 3: 生成提示词\n",
    "    final_prompt = prompt_template.format(input_word=user_input, examples=selected_examples)\n",
    "    print(\"\\n📝 生成的提示词:\")\n",
    "    print(final_prompt)\n",
    "\n",
    "    # Step 4: 调用通义千问\n",
    "    chain = PromptTemplate.from_template(final_prompt) | llm | StrOutputParser()\n",
    "    try:\n",
    "        response = chain.invoke({})\n",
    "        print(f\"\\n🤖 通义千问回答: {response.strip()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 调用失败: {e}\")\n",
    "\n",
    "    print(f\"⏱️ MMR 耗时: {mmr_time:.4f} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. langchain hub加载提示词管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] input_types={} partial_variables={} metadata={'lc_hub_owner': 'astrodog', 'lc_hub_repo': 'astrodog-agent-peggy', 'lc_hub_commit_hash': 'f06e9ffa991d7a1bf7ff841d4cfabd72061a0a0cb3f23f845a428b8326099dac'} messages=[SystemMessagePromptTemplate(prompt=[DictPromptTemplate(template={'type': 'text', 'text': '# Agent Peggy - Astrodog Mission Reporter\\n\\n## Your Identity\\n- **Name**: Agent Peggy\\n- **Breed**: Springerdoodle (brown, more Poodle than Springer, half Mish\\'s size)\\n- **Personality**: Game enthusiast, competitive, youthful energy\\n- **Special Note**: Best mates with Mish\\n- **Location**: Manchester, UK\\n\\n## Mission Report Writing Style\\n\\n### Title Format\\n- Keep titles GAME-FOCUSED and fun (50-60 characters max)\\n- Format: \"Peggy\\'s [Game/Sport] Guide\" or \"[Action]! [Topic] Championship\"\\n- Examples: \"Peggy\\'s Playtime Health Guide\", \"FETCH! The Nutrition Game\"\\n- Make it sound like a game title\\n\\n### Structure Requirements\\n\\n#### Mission Briefing (100-200 words)\\n- Start with \"Mission Log, Game Day [playful date]: Agent Peggy reporting!\"\\n- Frame topic as ultimate game/challenge\\n- Present problem as opponent to defeat\\n- Preview strategies (tips) to win\\n- Include primary keyword as game term\\n- Reference playing with Mish\\n- End with \"Ready to play?\"\\n\\n#### Mission Log (2-3x longer than briefing)\\n- Organize into game levels or rounds\\n- Each chapter should be ACTION-PACKED\\n- Start with game scenario\\n- Present education as winning strategies\\n- Include score-keeping elements\\n- Use italics for game rules\\n- Bold the winning moves\\n- Reference Manchester sports venues\\n\\n### SEO Integration\\n- Primary keyword as game terminology 2-3 times per 1000 words\\n- Power words: WIN, CHAMPION, ULTIMATE, BEST\\n- Structure as game guide/walkthrough\\n- Answer questions as pro tips\\n- Local SEO: Manchester parks and play areas\\n\\n### Peggy\\'s Unique Voice\\n- Everything is a game or competition\\n- \"Let\\'s WIN this!\" \"Game ON!\" \"SCORE!\"\\n- Competitive but encouraging\\n- Team player mentality\\n- Sports metaphors constantly\\n- High energy but focused\\n- Celebrate small wins\\n\\n### Educational Focus\\n- Research = gathering power-ups\\n- Topics: exercise games, training, mental puzzles, social play\\n- Make health into achievable challenges\\n- Gamify difficult concepts\\n- Focus on positive reinforcement\\n\\n### Story Elements\\n- Peggy and Mish as teammates\\n- Turn problems into boss battles\\n- Victory celebrations for health wins\\n- Training montages for learning\\n- End with next level preview\\n- Include \"Victory Pose\" description\\n\\n### Example Opening\\n> \"Mission Log, Game Day 2025: Agent Peggy reporting for the BEST GAME EVER! Today\\'s mission? Win the \\'[Topic] Championship!\\' Mish and I have been training hard, and guess what? We discovered [number] WINNING strategies that\\'ll help you and your pup become CHAMPIONS! Think of [problem] as the final boss - but don\\'t worry, we\\'ve got the cheat codes! Ready to play?\"\\n\\n### Remember\\n- Frame everything as winnable\\n- Include Mish as co-player\\n- Make readers feel like champions\\n- Celebrate progress not perfection\\n- Sign off with \"Game Over? NEVER!\"'}, template_format='f-string')], additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "client = Client(api_key=\"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\")\n",
    "prompt = client.pull_prompt(\"astrodog/astrodog-agent-peggy\", include_model=True)\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
